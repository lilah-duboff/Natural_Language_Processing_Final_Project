{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674299bb",
   "metadata": {},
   "source": [
    "# Comparing Natural Language Processing Approaches to Clustering Patents from Subsidiary Companies\n",
    "## Peter de Guzman (ped19)\n",
    "## Lilah DuBoff (lad90)\n",
    "## Christian Moreira (csm87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1af7e",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "Using a dataset of patents submitted to the U.S. Patent Office(USPTO) by subsidiaries of large multinational corporations, we will perform clustering of patents into patent topic categories. Some of the NLP techniques employed in this assignment include performing data cleaning on patent text, performing dimension reduction using PCA and machine learning algorithms(Multinomial Naive Bayes and Support Vector Classifier) for clustering patent abstracts and titles into a set of relevant comparable topics. The motivation behind this work is to address the task of tracking innovation across publicly traded companies, especially where patents are filed under different subsidiary names(i.e. “Google” with patents under “Waymo”, “DeepMind”, “Nest”); Emerging technological advancements often occur under subsidiaries of large corporations, but are not tracked due to the multitude of subsidiary firms. This project explores classification methods beyond the traditional Cooperative Patent Classification (CPC) system, offering more flexible and insightful ways for legal specialists, researchers, and investors to explore patent content and similar innovation strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca1790",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "Large publicly traded companies are constantly innovating and investing millions of dollars in research and development to maintain a competitive edge in the marketplace while developing new products. The patents during the innovation process are often filed by the subsidiaries of these large companies. Informed investors and market analysts must track the actions of these subsidiaries to better understand emerging trends and forecast growth across different industries, but manually tracking these can be resource and time intensive. \n",
    "\n",
    "To address this problem, we tested the ability of two models to effectively cluster patent abstracts and titles into meaningful groups by topic. We selected a multinomial Naive Bayes classifier and a support vector classifier (SVC) model. \n",
    "\n",
    "The **multinomial Naive Bayes classifier**….\n",
    "\n",
    "The **support vector classifier**…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37914ae",
   "metadata": {},
   "source": [
    "# Evaluation of Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4991422d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Load in libraries and data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3820f6ec",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pdeguz01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pdeguz01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING CODE\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# makes everything lowercase, removes punctuation, lemmatizes, and removes stopwords\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
    "    t = \" \".join([lemmatizer.lemmatize(word) for word in t.split() if word not in stop])\n",
    "    return t\n",
    "\n",
    "# load in new subset\n",
    "df_subset = pd.read_csv(\"data/top500_patents.csv\")\n",
    "\n",
    "# combine section and class, then clean text\n",
    "df_subset[\"Combined_ipc_clean\"] = (\n",
    "    df_subset[\"ipc_sections\"] + \"_\" + df_subset[\"ipc_classes\"].astype(str)\n",
    ")\n",
    "\n",
    "# combine title and abstract for easier classification\n",
    "df_subset[\"text_clean\"] = (\n",
    "    (df_subset[\"patent_title\"] + \": \" + df_subset[\"patent_abstract\"])\n",
    "    .astype(str)\n",
    "    .apply(clean_text)\n",
    ")\n",
    "\n",
    "#Additional Data Cleaning\n",
    "\n",
    "# drop under 50 observations\n",
    "df_subset = df_subset.groupby(\"Combined_ipc_clean\").filter(lambda x: len(x) >= 50)\n",
    "\n",
    "# remove the duplicate rows\n",
    "dups_to_remove = [\"H_4\", \"G_1\", \"B_1\", \"G_6\", \"C_7\"]\n",
    "for dup in dups_to_remove:\n",
    "    df_subset = df_subset[df_subset[\"Combined_ipc_clean\"] != dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06e65a12",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Synthetic Course Code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# List of IPC classes from your data\n",
    "classes = [\n",
    "    'G_06','H_04','A_61','A;C_61;7','A;C_07;61','C_07','G_16;6','G;H_4;6',\n",
    "    'H_02','G_02','H_01','A;C_07;12;61','C_07;12','A_63','A;G_6;63',\n",
    "    'G;H_04;06','G_10','B_65','F_16','G_01','B_26','B_29','A_43','A_46',\n",
    "    'A_46;61','B_60','H_03','B_67','C_12','G_06;10','B;C_1;10','C_10','B_01',\n",
    "    'E_21','C_08','C_11','F_02','G_1;6','B;C_1','B;C_1;7','A_23','F_01',\n",
    "    'G_10;6','B;G_6;60'\n",
    "]\n",
    "# Generate synthetic vocabulary for each class\n",
    "# We'll use 5-7 distinctive words per class\n",
    "class_vocab = {\n",
    "    'G_06': [\"network\", \"algorithm\", \"compute\", \"data\", \"process\", \"machine\"],\n",
    "    'H_04': [\"signal\", \"communication\", \"transmit\", \"channel\", \"frequency\", \"modulation\"],\n",
    "    'A_61': [\"medical\", \"device\", \"surgery\", \"treatment\", \"patient\", \"health\"],\n",
    "    'A;C_61;7': [\"chemical\", \"compound\", \"reaction\", \"acid\", \"solution\", \"synthesis\"],\n",
    "    'A;C_07;61': [\"drug\", \"therapy\", \"molecule\", \"pharma\", \"treatment\", \"dose\"],\n",
    "    'C_07': [\"organic\", \"reaction\", \"synthesis\", \"compound\", \"catalyst\", \"solution\"],\n",
    "    'G_16;6': [\"computer\", \"software\", \"data\", \"algorithm\", \"system\", \"processing\"],\n",
    "    'G;H_4;6': [\"network\", \"protocol\", \"signal\", \"transmission\", \"error\", \"coding\"],\n",
    "    'H_02': [\"telecom\", \"signal\", \"modulation\", \"channel\", \"data\", \"transmit\"],\n",
    "    'G_02': [\"imaging\", \"sensor\", \"signal\", \"measurement\", \"processing\", \"analysis\"],\n",
    "    'H_01': [\"electronics\", \"circuit\", \"voltage\", \"current\", \"device\", \"component\"],\n",
    "    'A;C_07;12;61': [\"compound\", \"reaction\", \"drug\", \"therapy\", \"molecule\", \"pharma\"],\n",
    "    'C_07;12': [\"synthesis\", \"organic\", \"compound\", \"reaction\", \"molecule\"],\n",
    "    'A_63': [\"game\", \"sport\", \"entertainment\", \"toy\", \"device\", \"play\"],\n",
    "    'A;G_6;63': [\"computer\", \"device\", \"software\", \"system\", \"interface\"],\n",
    "    'G;H_04;06': [\"signal\", \"communication\", \"network\", \"channel\", \"transmission\"],\n",
    "    'G_10': [\"mechanical\", \"machine\", \"engine\", \"device\", \"process\"],\n",
    "    'B_65': [\"packaging\", \"container\", \"material\", \"product\", \"process\"],\n",
    "    'F_16': [\"mechanical\", \"engine\", \"gear\", \"device\", \"machine\"],\n",
    "    'G_01': [\"measurement\", \"sensor\", \"instrument\", \"signal\", \"data\"],\n",
    "    'B_26': [\"metal\", \"alloy\", \"cutting\", \"process\", \"tool\"],\n",
    "    'B_29': [\"plastic\", \"molding\", \"material\", \"process\", \"product\"],\n",
    "    'A_43': [\"hair\", \"cosmetic\", \"care\", \"brush\", \"device\"],\n",
    "    'A_46': [\"clothing\", \"design\", \"fabric\", \"pattern\", \"material\"],\n",
    "    'A_46;61': [\"textile\", \"fabric\", \"sewing\", \"material\", \"design\"],\n",
    "    'B_60': [\"vehicle\", \"engine\", \"transport\", \"car\", \"wheel\"],\n",
    "    'H_03': [\"electronics\", \"circuit\", \"signal\", \"power\", \"device\"],\n",
    "    'B_67': [\"container\", \"tank\", \"liquid\", \"fluid\", \"pipe\"],\n",
    "    'C_12': [\"biotech\", \"enzyme\", \"cell\", \"microbe\", \"reaction\"],\n",
    "    'G_06;10': [\"software\", \"computer\", \"algorithm\", \"system\", \"data\"],\n",
    "    'B;C_1;10': [\"chemical\", \"process\", \"compound\", \"reaction\", \"material\"],\n",
    "    'C_10': [\"chemical\", \"reaction\", \"compound\", \"acid\", \"solution\"],\n",
    "    'B_01': [\"process\", \"material\", \"equipment\", \"reaction\", \"flow\"],\n",
    "    'E_21': [\"drilling\", \"oil\", \"well\", \"engine\", \"pump\"],\n",
    "    'C_08': [\"polymer\", \"material\", \"compound\", \"synthesis\", \"reaction\"],\n",
    "    'C_11': [\"oil\", \"chemical\", \"process\", \"refine\", \"compound\"],\n",
    "    'F_02': [\"engine\", \"turbine\", \"combustion\", \"mechanical\", \"airflow\"],\n",
    "    'G_1;6': [\"sensor\", \"signal\", \"measurement\", \"data\", \"processing\"],\n",
    "    'B;C_1': [\"chemical\", \"compound\", \"reaction\", \"process\", \"material\"],\n",
    "    'B;C_1;7': [\"chemical\", \"reaction\", \"compound\", \"process\", \"catalyst\"],\n",
    "    'A_23': [\"medical\", \"treatment\", \"therapy\", \"patient\", \"drug\"],\n",
    "    'F_01': [\"engine\", \"mechanical\", \"device\", \"combustion\", \"turbine\"],\n",
    "    'G_10;6': [\"mechanical\", \"device\", \"engine\", \"gear\", \"system\"],\n",
    "    'B;G_6;60': [\"process\", \"material\", \"engine\", \"chemical\", \"reaction\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f8d5faf",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Base class vocab (distinct words per class)\n",
    "class_vocab = {cls: [f\"{cls}_word{i}\" for i in range(5)] for cls in classes}\n",
    "\n",
    "# Shared words across all classes\n",
    "shared_words = [\"data\", \"system\", \"device\", \"process\", \"method\"]\n",
    "\n",
    "# Noise words (random filler)\n",
    "noise_words = [\"sample\", \"example\", \"info\", \"text\", \"random\"]\n",
    "\n",
    "# Add shared and noise words to class vocab\n",
    "for cls in classes:\n",
    "    class_vocab[cls] += shared_words\n",
    "    class_vocab[cls] += list(np.random.choice(noise_words, size=3))\n",
    "\n",
    "def generate_synthetic_text(classes, class_vocab, n_docs_per_class=100):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    all_words = [w for vocab in class_vocab.values() for w in vocab]\n",
    "\n",
    "    for cls in classes:\n",
    "        vocab = class_vocab[cls]\n",
    "\n",
    "        for _ in range(n_docs_per_class):\n",
    "            doc_length = np.random.randint(20, 40)  # shorter docs\n",
    "\n",
    "            # Only 30-40% words from class vocab\n",
    "            n_cls = int(doc_length * 0.35)\n",
    "            n_other = doc_length - n_cls\n",
    "\n",
    "            doc_words = list(np.random.choice(vocab, size=n_cls, replace=True))\n",
    "\n",
    "            # 65% of words are from other classes/noise\n",
    "            doc_words += list(np.random.choice(all_words, size=n_other, replace=True))\n",
    "\n",
    "            np.random.shuffle(doc_words)\n",
    "            texts.append(\" \".join(doc_words))\n",
    "            labels.append(cls)\n",
    "\n",
    "    return pd.Series(texts), pd.Series(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X_syn, y_syn = generate_synthetic_text(classes, class_vocab, n_docs_per_class=100)\n",
    "#print(\"Synthetic dataset size:\", len(X_syn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cc33706",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| A;C_07;12;61 |      0.72   |   0.9    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_07;61    |      1      |   0.35   |     0.5185 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_61;7     |      0.8824 |   0.75   |     0.8108 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;G_6;63     |      0.7391 |   0.85   |     0.7907 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_23         |      0.9    |   0.9    |     0.9    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_43         |      0.8182 |   0.9    |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46         |      0.9444 |   0.85   |     0.8947 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46;61      |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_61         |      0.8696 |   1      |     0.9302 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_63         |      0.9091 |   1      |     0.9524 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1        |      0.9524 |   1      |     0.9756 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;10     |      1      |   0.7    |     0.8235 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;7      |      0.9444 |   0.85   |     0.8947 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;G_6;60     |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_01         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_26         |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_29         |      1      |   0.95   |     0.9744 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_60         |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_65         |      0.9048 |   0.95   |     0.9268 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_67         |      0.8333 |   0.75   |     0.7895 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07         |      0.8333 |   1      |     0.9091 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07;12      |      0.9412 |   0.8    |     0.8649 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_08         |      0.8421 |   0.8    |     0.8205 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_10         |      0.95   |   0.95   |     0.95   |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_11         |      0.8333 |   0.75   |     0.7895 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_12         |      0.8261 |   0.95   |     0.8837 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| E_21         |      0.7619 |   0.8    |     0.7805 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_01         |      0.8    |   1      |     0.8889 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_02         |      0.8824 |   0.75   |     0.8108 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_16         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_04;06    |      0.6552 |   0.95   |     0.7755 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_4;6      |      1      |   0.9    |     0.9474 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_01         |      0.72   |   0.9    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_02         |      0.8    |   0.8    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06         |      0.9412 |   0.8    |     0.8649 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06;10      |      1      |   0.95   |     0.9744 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10         |      1      |   0.75   |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10;6       |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_16;6       |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_1;6        |      1      |   0.75   |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_01         |      1      |   0.9    |     0.9474 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_02         |      0.8696 |   1      |     0.9302 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_03         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_04         |      0.7826 |   0.9    |     0.8372 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |      0.8648 |   0.8648 |     0.8648 |    0.8648 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |      0.879  |   0.8648 |     0.863  |  880      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |      0.879  |   0.8648 |     0.863  |  880      |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "X_syn, y_syn = generate_synthetic_text(classes, class_vocab, n_docs_per_class=100)\n",
    "\n",
    "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.2, stratify=y_syn, random_state=42\n",
    ")\n",
    "\n",
    "# Naive Bayes pipeline\n",
    "class_model_syn = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=5000)),\n",
    "    (\"nb\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Train\n",
    "class_model_syn.fit(X_train_syn, y_train_syn)\n",
    "preds_syn = class_model_syn.predict(X_test_syn)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#print(\"Synthetic Data Accuracy:\", accuracy_score(y_test_syn, preds_syn))\n",
    "#print(classification_report(y_test_syn, preds_syn))\n",
    "\n",
    "# Compute report as dict\n",
    "report_dict = classification_report(y_test_syn, preds_syn, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Round numbers for readability\n",
    "report_df = report_df.round(4)\n",
    "\n",
    "print(tabulate(report_df, headers='keys', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e839c",
   "metadata": {},
   "source": [
    "\n",
    "**Multinomial Naive Bayes Classifier:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Support Vector Classifier:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94b7fa",
   "metadata": {},
   "source": [
    "# Application of Solution on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60fd9e",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df80b22a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPC Class Accuracy: 0.6889058330156309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "A;C_07;12;61       0.00      0.00      0.00        13\n",
      "   A;C_07;61       0.50      0.83      0.63       103\n",
      "    A;C_61;7       0.73      0.70      0.71        73\n",
      "    A;G_6;63       1.00      0.09      0.17        11\n",
      "        A_23       0.91      0.83      0.87        12\n",
      "        A_43       0.85      0.98      0.91        51\n",
      "        A_46       0.50      0.13      0.21        15\n",
      "     A_46;61       0.83      0.36      0.50        14\n",
      "        A_61       0.79      0.90      0.84       497\n",
      "        A_63       0.89      0.89      0.89        44\n",
      "       B;C_1       0.54      0.70      0.61        10\n",
      "    B;C_1;10       0.69      0.35      0.46        26\n",
      "     B;C_1;7       1.00      0.20      0.33        20\n",
      "    B;G_6;60       0.00      0.00      0.00        10\n",
      "        B_01       0.38      0.14      0.20        22\n",
      "        B_26       0.88      0.84      0.86        58\n",
      "        B_29       0.89      0.85      0.87        20\n",
      "        B_60       0.73      0.73      0.73        22\n",
      "        B_65       0.93      0.72      0.81        57\n",
      "        B_67       0.75      0.69      0.72        13\n",
      "        C_07       0.33      0.20      0.25        64\n",
      "     C_07;12       0.41      0.32      0.36        22\n",
      "        C_08       1.00      0.47      0.64        15\n",
      "        C_10       0.52      0.93      0.67        73\n",
      "        C_11       0.82      0.74      0.78        19\n",
      "        C_12       0.43      0.27      0.33        33\n",
      "        E_21       0.89      0.72      0.80        54\n",
      "        F_01       0.86      0.67      0.75        18\n",
      "        F_02       0.73      0.55      0.63        20\n",
      "        F_16       1.00      0.16      0.27        19\n",
      "   G;H_04;06       0.88      0.08      0.14        92\n",
      "     G;H_4;6       0.88      0.38      0.53       146\n",
      "        G_01       0.71      0.64      0.68        76\n",
      "        G_02       0.75      0.48      0.59        31\n",
      "        G_06       0.60      0.89      0.72       434\n",
      "     G_06;10       0.00      0.00      0.00        15\n",
      "        G_10       0.70      0.62      0.66        34\n",
      "      G_10;6       0.00      0.00      0.00        14\n",
      "      G_16;6       0.00      0.00      0.00        21\n",
      "       G_1;6       0.67      0.15      0.25        13\n",
      "        H_01       0.88      0.62      0.73        48\n",
      "        H_02       0.50      0.18      0.27        11\n",
      "        H_03       1.00      0.18      0.31        11\n",
      "        H_04       0.68      0.83      0.75       249\n",
      "\n",
      "    accuracy                           0.69      2623\n",
      "   macro avg       0.66      0.48      0.51      2623\n",
      "weighted avg       0.70      0.69      0.65      2623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes Classifier\n",
    "# Inputs and labels\n",
    "X_bayes = df_subset[\"text_clean\"]\n",
    "y_bayes = df_subset[\"Combined_ipc_clean\"]\n",
    "\n",
    "X_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = train_test_split(\n",
    "    X_bayes, y_bayes, test_size=0.20, stratify=y_bayes, random_state=42\n",
    ")\n",
    "\n",
    "# Build model\n",
    "class_model = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=50000)),\n",
    "        (\"nb\", ComplementNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train\n",
    "class_model.fit(X_train_bayes, y_train_bayes)\n",
    "\n",
    "# Evaluate\n",
    "preds = class_model.predict(X_test_bayes)\n",
    "print(\"IPC Class Accuracy:\", accuracy_score(y_test_bayes, preds))\n",
    "print(classification_report(y_test_bayes, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffddd5b0",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# # Predictions\n",
    "# y_pred = class_model.predict(X_test_bayes)\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_test_bayes, y_pred, labels=class_model.classes_)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(\n",
    "#     cm,\n",
    "#     annot=False,\n",
    "#     cmap=\"Blues\",\n",
    "#     xticklabels=class_model.classes_,\n",
    "#     yticklabels=class_model.classes_,\n",
    "# )\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80848bf",
   "metadata": {},
   "source": [
    "# Pros and Cons of the Solution:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
