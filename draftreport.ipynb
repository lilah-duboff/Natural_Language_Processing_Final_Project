{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674299bb",
   "metadata": {},
   "source": [
    "# Comparing Natural Language Processing Approaches to Clustering Patents from Subsidiary Companies\n",
    "## Peter de Guzman (ped19)\n",
    "## Lilah DuBoff (lad90)\n",
    "## Christian Moreira (csm87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1af7e",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "Using a dataset of patents submitted to the U.S. Patent Office(USPTO) by subsidiaries of large multinational corporations, we will perform clustering of patents into patent topic categories. Some of the NLP techniques employed in this assignment include performing data cleaning on patent text, performing dimension reduction using PCA and machine learning algorithms(Multinomial Naive Bayes and Support Vector Classifier) for clustering patent abstracts and titles into a set of relevant comparable topics. The motivation behind this work is to address the task of tracking innovation across publicly traded companies, especially where patents are filed under different subsidiary names(i.e. “Google” with patents under “Waymo”, “DeepMind”, “Nest”); Emerging technological advancements often occur under subsidiaries of large corporations, but are not tracked due to the multitude of subsidiary firms. This project explores classification methods beyond the traditional Cooperative Patent Classification (CPC) system, offering more flexible and insightful ways for legal specialists, researchers, and investors to explore patent content and similar innovation strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca1790",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "Large publicly traded companies are constantly innovating and investing millions of dollars in research and development to maintain a competitive edge in the marketplace while developing new products. The patents during the innovation process are often filed by the subsidiaries of these large companies. Informed investors and market analysts must track the actions of these subsidiaries to better understand emerging trends and forecast growth across different industries, but manually tracking these can be resource and time intensive. \n",
    "\n",
    "To address this problem, we tested the ability of two models to effectively cluster patent abstracts and titles into meaningful groups by topic. We selected a multinomial Naive Bayes classifier and a support vector classifier (SVC) model. \n",
    "\n",
    "The **multinomial Naive Bayes classifier**….\n",
    "\n",
    "The **support vector classifier**…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37914ae",
   "metadata": {},
   "source": [
    "# Evaluation of Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4991422d",
   "metadata": {
    "tags": [
     "hide-input",
     "no-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Load in libraries and data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3820f6ec",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING CODE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# makes everything lowercase, removes punctuation, lemmatizes, and removes stopwords\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
    "    t = \" \".join([lemmatizer.lemmatize(word) for word in t.split() if word not in stop])\n",
    "    return t\n",
    "\n",
    "# load in new subset\n",
    "df_subset = pd.read_csv(\"data/top500_patents.csv\")\n",
    "\n",
    "# combine section and class, then clean text\n",
    "df_subset[\"Combined_ipc_clean\"] = (\n",
    "    df_subset[\"ipc_sections\"] + \"_\" + df_subset[\"ipc_classes\"].astype(str)\n",
    ")\n",
    "\n",
    "# combine title and abstract for easier classification\n",
    "df_subset[\"text_clean\"] = (\n",
    "    (df_subset[\"patent_title\"] + \": \" + df_subset[\"patent_abstract\"])\n",
    "    .astype(str)\n",
    "    .apply(clean_text)\n",
    ")\n",
    "\n",
    "#Additional Data Cleaning\n",
    "\n",
    "# drop under 50 observations\n",
    "df_subset = df_subset.groupby(\"Combined_ipc_clean\").filter(lambda x: len(x) >= 50)\n",
    "\n",
    "# remove the duplicate rows\n",
    "dups_to_remove = [\"H_4\", \"G_1\", \"B_1\", \"G_6\", \"C_7\"]\n",
    "for dup in dups_to_remove:\n",
    "    df_subset = df_subset[df_subset[\"Combined_ipc_clean\"] != dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e65a12",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Synthetic Code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# List of IPC classes from your data\n",
    "classes = [\n",
    "    'G_06','H_04','A_61','A;C_61;7','A;C_07;61','C_07','G_16;6','G;H_4;6',\n",
    "    'H_02','G_02','H_01','A;C_07;12;61','C_07;12','A_63','A;G_6;63',\n",
    "    'G;H_04;06','G_10','B_65','F_16','G_01','B_26','B_29','A_43','A_46',\n",
    "    'A_46;61','B_60','H_03','B_67','C_12','G_06;10','B;C_1;10','C_10','B_01',\n",
    "    'E_21','C_08','C_11','F_02','G_1;6','B;C_1','B;C_1;7','A_23','F_01',\n",
    "    'G_10;6','B;G_6;60'\n",
    "]\n",
    "# Generate synthetic vocabulary for each class\n",
    "# We'll use 5-7 distinctive words per class\n",
    "class_vocab = {\n",
    "    'G_06': [\"network\", \"algorithm\", \"compute\", \"data\", \"process\", \"machine\"],\n",
    "    'H_04': [\"signal\", \"communication\", \"transmit\", \"channel\", \"frequency\", \"modulation\"],\n",
    "    'A_61': [\"medical\", \"device\", \"surgery\", \"treatment\", \"patient\", \"health\"],\n",
    "    'A;C_61;7': [\"chemical\", \"compound\", \"reaction\", \"acid\", \"solution\", \"synthesis\"],\n",
    "    'A;C_07;61': [\"drug\", \"therapy\", \"molecule\", \"pharma\", \"treatment\", \"dose\"],\n",
    "    'C_07': [\"organic\", \"reaction\", \"synthesis\", \"compound\", \"catalyst\", \"solution\"],\n",
    "    'G_16;6': [\"computer\", \"software\", \"data\", \"algorithm\", \"system\", \"processing\"],\n",
    "    'G;H_4;6': [\"network\", \"protocol\", \"signal\", \"transmission\", \"error\", \"coding\"],\n",
    "    'H_02': [\"telecom\", \"signal\", \"modulation\", \"channel\", \"data\", \"transmit\"],\n",
    "    'G_02': [\"imaging\", \"sensor\", \"signal\", \"measurement\", \"processing\", \"analysis\"],\n",
    "    'H_01': [\"electronics\", \"circuit\", \"voltage\", \"current\", \"device\", \"component\"],\n",
    "    'A;C_07;12;61': [\"compound\", \"reaction\", \"drug\", \"therapy\", \"molecule\", \"pharma\"],\n",
    "    'C_07;12': [\"synthesis\", \"organic\", \"compound\", \"reaction\", \"molecule\"],\n",
    "    'A_63': [\"game\", \"sport\", \"entertainment\", \"toy\", \"device\", \"play\"],\n",
    "    'A;G_6;63': [\"computer\", \"device\", \"software\", \"system\", \"interface\"],\n",
    "    'G;H_04;06': [\"signal\", \"communication\", \"network\", \"channel\", \"transmission\"],\n",
    "    'G_10': [\"mechanical\", \"machine\", \"engine\", \"device\", \"process\"],\n",
    "    'B_65': [\"packaging\", \"container\", \"material\", \"product\", \"process\"],\n",
    "    'F_16': [\"mechanical\", \"engine\", \"gear\", \"device\", \"machine\"],\n",
    "    'G_01': [\"measurement\", \"sensor\", \"instrument\", \"signal\", \"data\"],\n",
    "    'B_26': [\"metal\", \"alloy\", \"cutting\", \"process\", \"tool\"],\n",
    "    'B_29': [\"plastic\", \"molding\", \"material\", \"process\", \"product\"],\n",
    "    'A_43': [\"hair\", \"cosmetic\", \"care\", \"brush\", \"device\"],\n",
    "    'A_46': [\"clothing\", \"design\", \"fabric\", \"pattern\", \"material\"],\n",
    "    'A_46;61': [\"textile\", \"fabric\", \"sewing\", \"material\", \"design\"],\n",
    "    'B_60': [\"vehicle\", \"engine\", \"transport\", \"car\", \"wheel\"],\n",
    "    'H_03': [\"electronics\", \"circuit\", \"signal\", \"power\", \"device\"],\n",
    "    'B_67': [\"container\", \"tank\", \"liquid\", \"fluid\", \"pipe\"],\n",
    "    'C_12': [\"biotech\", \"enzyme\", \"cell\", \"microbe\", \"reaction\"],\n",
    "    'G_06;10': [\"software\", \"computer\", \"algorithm\", \"system\", \"data\"],\n",
    "    'B;C_1;10': [\"chemical\", \"process\", \"compound\", \"reaction\", \"material\"],\n",
    "    'C_10': [\"chemical\", \"reaction\", \"compound\", \"acid\", \"solution\"],\n",
    "    'B_01': [\"process\", \"material\", \"equipment\", \"reaction\", \"flow\"],\n",
    "    'E_21': [\"drilling\", \"oil\", \"well\", \"engine\", \"pump\"],\n",
    "    'C_08': [\"polymer\", \"material\", \"compound\", \"synthesis\", \"reaction\"],\n",
    "    'C_11': [\"oil\", \"chemical\", \"process\", \"refine\", \"compound\"],\n",
    "    'F_02': [\"engine\", \"turbine\", \"combustion\", \"mechanical\", \"airflow\"],\n",
    "    'G_1;6': [\"sensor\", \"signal\", \"measurement\", \"data\", \"processing\"],\n",
    "    'B;C_1': [\"chemical\", \"compound\", \"reaction\", \"process\", \"material\"],\n",
    "    'B;C_1;7': [\"chemical\", \"reaction\", \"compound\", \"process\", \"catalyst\"],\n",
    "    'A_23': [\"medical\", \"treatment\", \"therapy\", \"patient\", \"drug\"],\n",
    "    'F_01': [\"engine\", \"mechanical\", \"device\", \"combustion\", \"turbine\"],\n",
    "    'G_10;6': [\"mechanical\", \"device\", \"engine\", \"gear\", \"system\"],\n",
    "    'B;G_6;60': [\"process\", \"material\", \"engine\", \"chemical\", \"reaction\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8d5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class vocab (distinct words per class)\n",
    "class_vocab = {cls: [f\"{cls}_word{i}\" for i in range(5)] for cls in classes}\n",
    "\n",
    "# Shared words across all classes\n",
    "shared_words = [\"data\", \"system\", \"device\", \"process\", \"method\"]\n",
    "\n",
    "# Noise words (random filler)\n",
    "noise_words = [\"sample\", \"example\", \"info\", \"text\", \"random\"]\n",
    "\n",
    "# Add shared and noise words to class vocab\n",
    "for cls in classes:\n",
    "    class_vocab[cls] += shared_words\n",
    "    class_vocab[cls] += list(np.random.choice(noise_words, size=3))\n",
    "\n",
    "def generate_synthetic_text(classes, class_vocab, n_docs_per_class=100):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    all_words = [w for vocab in class_vocab.values() for w in vocab]\n",
    "\n",
    "    for cls in classes:\n",
    "        vocab = class_vocab[cls]\n",
    "\n",
    "        for _ in range(n_docs_per_class):\n",
    "            doc_length = np.random.randint(20, 40)  # shorter docs\n",
    "\n",
    "            # Only 30-40% words from class vocab\n",
    "            n_cls = int(doc_length * 0.35)\n",
    "            n_other = doc_length - n_cls\n",
    "\n",
    "            doc_words = list(np.random.choice(vocab, size=n_cls, replace=True))\n",
    "\n",
    "            # 65% of words are from other classes/noise\n",
    "            doc_words += list(np.random.choice(all_words, size=n_other, replace=True))\n",
    "\n",
    "            np.random.shuffle(doc_words)\n",
    "            texts.append(\" \".join(doc_words))\n",
    "            labels.append(cls)\n",
    "\n",
    "    return pd.Series(texts), pd.Series(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X_syn, y_syn = generate_synthetic_text(classes, class_vocab, n_docs_per_class=100)\n",
    "#print(\"Synthetic dataset size:\", len(X_syn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc33706",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| A;C_07;12;61 |      0.72   |   0.9    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_07;61    |      1      |   0.35   |     0.5185 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_61;7     |      0.8824 |   0.75   |     0.8108 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;G_6;63     |      0.7391 |   0.85   |     0.7907 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_23         |      0.9    |   0.9    |     0.9    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_43         |      0.8182 |   0.9    |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46         |      0.9444 |   0.85   |     0.8947 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46;61      |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_61         |      0.8696 |   1      |     0.9302 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_63         |      0.9091 |   1      |     0.9524 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1        |      0.9524 |   1      |     0.9756 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;10     |      1      |   0.7    |     0.8235 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;7      |      0.9444 |   0.85   |     0.8947 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;G_6;60     |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_01         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_26         |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_29         |      1      |   0.95   |     0.9744 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_60         |      0.8571 |   0.9    |     0.878  |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_65         |      0.9048 |   0.95   |     0.9268 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_67         |      0.8333 |   0.75   |     0.7895 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07         |      0.8333 |   1      |     0.9091 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07;12      |      0.9412 |   0.8    |     0.8649 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_08         |      0.8421 |   0.8    |     0.8205 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_10         |      0.95   |   0.95   |     0.95   |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_11         |      0.8333 |   0.75   |     0.7895 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_12         |      0.8261 |   0.95   |     0.8837 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| E_21         |      0.7619 |   0.8    |     0.7805 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_01         |      0.8    |   1      |     0.8889 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_02         |      0.8824 |   0.75   |     0.8108 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_16         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_04;06    |      0.6552 |   0.95   |     0.7755 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_4;6      |      1      |   0.9    |     0.9474 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_01         |      0.72   |   0.9    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_02         |      0.8    |   0.8    |     0.8    |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06         |      0.9412 |   0.8    |     0.8649 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06;10      |      1      |   0.95   |     0.9744 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10         |      1      |   0.75   |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10;6       |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_16;6       |      1      |   0.85   |     0.9189 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_1;6        |      1      |   0.75   |     0.8571 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_01         |      1      |   0.9    |     0.9474 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_02         |      0.8696 |   1      |     0.9302 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_03         |      0.75   |   0.9    |     0.8182 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_04         |      0.7826 |   0.9    |     0.8372 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |      0.8648 |   0.8648 |     0.8648 |    0.8648 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |      0.879  |   0.8648 |     0.863  |  880      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |      0.879  |   0.8648 |     0.863  |  880      |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "X_syn, y_syn = generate_synthetic_text(classes, class_vocab, n_docs_per_class=100)\n",
    "\n",
    "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.2, stratify=y_syn, random_state=42\n",
    ")\n",
    "\n",
    "# Naive Bayes pipeline\n",
    "class_model_syn = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=5000)),\n",
    "    (\"nb\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Train\n",
    "class_model_syn.fit(X_train_syn, y_train_syn)\n",
    "preds_syn = class_model_syn.predict(X_test_syn)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#print(\"Synthetic Data Accuracy:\", accuracy_score(y_test_syn, preds_syn))\n",
    "#print(classification_report(y_test_syn, preds_syn))\n",
    "\n",
    "# Compute report as dict\n",
    "report_dict = classification_report(y_test_syn, preds_syn, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Round numbers for readability\n",
    "report_df = report_df.round(4)\n",
    "\n",
    "print(tabulate(report_df, headers='keys', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e839c",
   "metadata": {},
   "source": [
    "\n",
    "**Multinomial Naive Bayes Classifier:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Support Vector Classifier:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94b7fa",
   "metadata": {},
   "source": [
    "# Application of Solution on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9d361",
   "metadata": {},
   "source": [
    "To conduct this experiment, we collected publicly available patent data from the U.S. Patent and Trademark Office (USPTO). First, we researched the thirty companies listed in the Dow Jones Industrial Average, a stock market index of prominent companies. Referencing this list, we collected the legal incorporated names of subsidiaries for each large company. Finally, we used this list of subsidiary names as input for the USPTO API which returned the patent title and abstract for each subsidiary company. \n",
    "\n",
    "We conducted multiple data pre-processing steps to clean the data and improve accuracy. First, we removed duplicate patent observations. We also combined the “class” and “section” labels for each patent to produce more meaningful clusters. Both the “class” and “section” labels identify patents by industry field and topic. We additionally subset the data to only predict classes that have more than fifty observations. As patent classifications can be very niche, we expect to observe many patents that have multiple classifications but only one or two observations. This sort of data does not typically perform well for machine learning problems, so we dropped them. \n",
    "\n",
    "We also performed multiple steps to make the text easier to classify. We first converted all text to lowercase and used the Python Natural Language Toolkit (“nltk”) package to remove stopwords such as “the”, “and”, etc. We also used a lemmatizer which converts all the words in our dataset to their dictionary form (a lemma). This improves the accuracy by treating words with similar meanings the same, reducing data redundancy and making the text more consistent. Finally, to quicken the training process and reduce the compute load on our machines, we reduced the dataset to the most recent 500 patents. We then created training and test splits from this smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60fd9e",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes Classifier:**\n",
    "\n",
    "After these preprocessing steps, the multinomial Naive Bayes classifier achieved an overall accuracy of 69%. Given the 47 classes in our dataset and the imbalanced nature of patent classification, this performance exceeded our expectations. \n",
    "\n",
    "The macro average F1 score, which treats classes equally, was 51%. This indicates that we have imbalance issues in our dataset. The weighted F1 score, which accounts for imbalance by weighing each class by how frequent it is, was 65%. Since the weighted average is high, we can conclude that the model performed well on classes that are more common. However, the macro average was much lower, displaying that the model performs poorly on rare classes. When reviewing the classification report below, we can observe that the F1 score was consistently higher for classes with more support, or a higher number of true samples in the class. This matches our expectation that the model’s metrics are more stable for classes with more support. Poor results occurred more often for classes with support of fewer than 20 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df80b22a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| A;C_07;12;61 |      0      |   0      |     0      |   13      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_07;61    |      0.5    |   0.835  |     0.6255 |  103      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;C_61;7     |      0.7286 |   0.6986 |     0.7133 |   73      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A;G_6;63     |      1      |   0.0909 |     0.1667 |   11      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_23         |      0.9091 |   0.8333 |     0.8696 |   12      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_43         |      0.8475 |   0.9804 |     0.9091 |   51      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46         |      0.5    |   0.1333 |     0.2105 |   15      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_46;61      |      0.8333 |   0.3571 |     0.5    |   14      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_61         |      0.7866 |   0.8974 |     0.8383 |  497      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| A_63         |      0.8864 |   0.8864 |     0.8864 |   44      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1        |      0.5385 |   0.7    |     0.6087 |   10      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;10     |      0.6923 |   0.3462 |     0.4615 |   26      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;C_1;7      |      1      |   0.2    |     0.3333 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B;G_6;60     |      0      |   0      |     0      |   10      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_01         |      0.375  |   0.1364 |     0.2    |   22      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_26         |      0.875  |   0.8448 |     0.8596 |   58      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_29         |      0.8947 |   0.85   |     0.8718 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_60         |      0.7273 |   0.7273 |     0.7273 |   22      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_65         |      0.9318 |   0.7193 |     0.8119 |   57      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| B_67         |      0.75   |   0.6923 |     0.72   |   13      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07         |      0.3333 |   0.2031 |     0.2524 |   64      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_07;12      |      0.4118 |   0.3182 |     0.359  |   22      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_08         |      1      |   0.4667 |     0.6364 |   15      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_10         |      0.5191 |   0.9315 |     0.6667 |   73      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_11         |      0.8235 |   0.7368 |     0.7778 |   19      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| C_12         |      0.4286 |   0.2727 |     0.3333 |   33      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| E_21         |      0.8864 |   0.7222 |     0.7959 |   54      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_01         |      0.8571 |   0.6667 |     0.75   |   18      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_02         |      0.7333 |   0.55   |     0.6286 |   20      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| F_16         |      1      |   0.1579 |     0.2727 |   19      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_04;06    |      0.875  |   0.0761 |     0.14   |   92      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G;H_4;6      |      0.875  |   0.3836 |     0.5333 |  146      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_01         |      0.7101 |   0.6447 |     0.6759 |   76      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_02         |      0.75   |   0.4839 |     0.5882 |   31      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06         |      0.596  |   0.894  |     0.7152 |  434      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_06;10      |      0      |   0      |     0      |   15      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10         |      0.7    |   0.6176 |     0.6562 |   34      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_10;6       |      0      |   0      |     0      |   14      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_16;6       |      0      |   0      |     0      |   21      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| G_1;6        |      0.6667 |   0.1538 |     0.25   |   13      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_01         |      0.8824 |   0.625  |     0.7317 |   48      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_02         |      0.5    |   0.1818 |     0.2667 |   11      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_03         |      1      |   0.1818 |     0.3077 |   11      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| H_04         |      0.6787 |   0.8313 |     0.7473 |  249      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| accuracy     |      0.6889 |   0.6889 |     0.6889 |    0.6889 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |      0.6592 |   0.4779 |     0.5091 | 2623      |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |      0.6986 |   0.6889 |     0.6539 | 2623      |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes Classifier\n",
    "# Inputs and labels\n",
    "X_bayes = df_subset[\"text_clean\"]\n",
    "y_bayes = df_subset[\"Combined_ipc_clean\"]\n",
    "\n",
    "X_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = train_test_split(\n",
    "    X_bayes, y_bayes, test_size=0.20, stratify=y_bayes, random_state=42\n",
    ")\n",
    "\n",
    "# Build model\n",
    "class_model = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=50000)),\n",
    "        (\"nb\", ComplementNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train\n",
    "class_model.fit(X_train_bayes, y_train_bayes)\n",
    "\n",
    "# Evaluate\n",
    "preds = class_model.predict(X_test_bayes)\n",
    "#print(\"IPC Class Accuracy:\", accuracy_score(y_test_bayes, preds))\n",
    "#print(classification_report(y_test_bayes, preds))\n",
    "\n",
    "# Compute report as dict\n",
    "report_dict_realdata = classification_report(y_test_bayes, preds, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "report_df_real = pd.DataFrame(report_dict_realdata).transpose()\n",
    "\n",
    "# Round numbers for readability\n",
    "report_df_real = report_df_real.round(4)\n",
    "\n",
    "print(tabulate(report_df_real, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffddd5b0",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# # Predictions\n",
    "# y_pred = class_model.predict(X_test_bayes)\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_test_bayes, y_pred, labels=class_model.classes_)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(\n",
    "#     cm,\n",
    "#     annot=False,\n",
    "#     cmap=\"Blues\",\n",
    "#     xticklabels=class_model.classes_,\n",
    "#     yticklabels=class_model.classes_,\n",
    "# )\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80848bf",
   "metadata": {},
   "source": [
    "# Pros and Cons of the Solution:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
